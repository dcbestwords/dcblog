import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{o as s,c as a,e as t}from"./app-ClJ_1Ydt.js";const p="/dcblog/assets/local_minimum-BYhjLrU-.svg",e="/dcblog/assets/saddle-vClhm6ni.svg",o="/dcblog/assets/gradient-tiKjipcT.svg",c="/dcblog/assets/image-20230329155746146-B91yW8yb.png",l="/dcblog/assets/attention-output-CwOucdKr.svg",i={},u=t(`<h2 id="一、导入数据集" tabindex="-1"><a class="header-anchor" href="#一、导入数据集"><span>一、导入数据集</span></a></h2><h3 id="_1-处理数据集" tabindex="-1"><a class="header-anchor" href="#_1-处理数据集"><span>1. 处理数据集</span></a></h3><blockquote><p><code>torch.utils.data.TensorDataset(*tensors)</code></p></blockquote><p>这是 PyTorch 中的一个数据集类，用于针对<strong>张量数据</strong>创建数据集，即<strong style="color:red;">将张量表示的数据封装为数据集对象</strong>，以便于之后用于训练模型。</p><p>该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签，则需要使用两个张量来描述数据集，一个张量表示图像数据，另一个张量表示标签数据。</p><p>该类的初始化函数将所有参数合并为一个张量元组，即 <code>(*tensors)</code>，同时还可以定义一些额外的参数，例如 <code>transform=None</code> 用于对数据进行转换，<code>target_transform=None</code> 用于对标签进行转换。</p><p>例如，以下代码创建了一个 <code>TensorDataset</code> 对象，包含数据 <code>X</code> 和标签 <code>y</code>：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们创建了一个大小为 <code>(100, 10)</code> 的张量 <code>X</code>，和一个大小为 <code>(100,)</code> 的张量 <code>y</code>，分别代表输入数据和标签。然后，我们创建了一个 <code>TensorDataset</code> 对象 <code>ds</code>，将 <code>X</code> 和 <code>y</code> 作为参数传入。这样，我们就将数据 <code>X</code> 与标签 <code>y</code> 封装为了一个 PyTorch 的数据集对象，即 <code>ds</code>。</p><blockquote><p><code>torch.utils.data.Dataset</code></p></blockquote><p>这是一个抽象类，用作自定义数据集的基础。如果要<strong>自定义数据集</strong>，需要继承它并实现其中的两个方法：<code>len()</code> 和 <code>getItem()</code>。</p><p><code>len()</code> 方法返回数据集中的样本数量，<code>getitem()</code> 方法传递样本索引并返回样本及其标签（如果不需要标签，则可以选择省略）。在<code>__init__</code>方法中，我们可以将数据集读取到内存中，可以使用Pandas库等来读取CSV格式的数据，也可以通过自己编写函数读取图像数据。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image

<span class="token keyword">class</span> <span class="token class-name">MyCustomDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_file<span class="token punctuation">,</span> root_dir<span class="token punctuation">,</span> transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        csv_file (string): 文件路径，包括标签和图像信息。
        root_dir (string): 图像所在的目录。
        transform (callable, optional): 可选的图片转换(自定义或PyTorch中的)操作来增强数据。
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>annotations <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>csv_file<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>root_dir <span class="token operator">=</span> root_dir
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>annotations<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        img_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>root_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>annotations<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
        label <span class="token operator">=</span> self<span class="token punctuation">.</span>annotations<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>
            image <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在训练模型时，我们可以通过 <code>DataLoader</code> 类来使用 <code>TensorDataset</code> 或者<code>Dataset</code>对象，从而实现批量加载数据，并且可以进行数据增强和多线程加载等操作。</li></ul><blockquote><p><code>torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False)</code></p></blockquote><p>这是 PyTorch 中用于<strong>数据加载</strong>的工具类，用于<strong style="color:red;">在训练过程中批量加载数据，进行数据增强、数据划分和多线程等操作</strong>。</p><ul><li><code>dataset</code>: 加载数据所在的数据集。</li><li><code>batch_size</code>: 批处理大小。</li><li><code>shuffle</code>: 是否打乱数据。</li><li><code>sampler</code>: 样本抽样方式，可以自定义。</li><li><code>num_workers</code>: 线程的数量，0表示不使用多线程。</li><li><code>collate_fn</code>: 自定义的 batch 操作。</li><li><code>pin_memory</code>: 是否将数据保存到 CUDA 的固定内存上，提高 GPU 效率。</li><li><code>drop_last</code>: 如果 dataset 中数据总数不能被 batch_size 整除，则 drop_last 如果为 True，则将多余的数据删除；否则放到最后一个 batch 里去。</li></ul><p>例如，以下代码展示了如何使用 <code>DataLoader</code> 加载一个 <code>TensorDataset</code> 对象：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span> DataLoader

X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> 

loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token comment"># 进行训练或推理的代码    pass </span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先创建了一个大小为 <code>(100, 10)</code> 的数据张量 <code>X</code> 和大小为 <code>(100,)</code> 的标签张量 <code>y</code>，然后使用 <code>TensorDataset</code> 将这两个张量封装成一个数据集 <code>ds</code>。</p><p>接着，我们使用 <code>DataLoader</code> 对象 <code>loader</code> 加载数据集，批处理大小为 10，shuffle 为 True 表示每个 epoch 都会对数据进行随机打乱。在训练模型时，我们可以通过 <code>enumerate</code> 函数对 <code>loader</code> 进行迭代，通过 <code>(data, target)</code> 获取每个批次的输入数据和标签数据，进行模型的训练和推理。</p><p>以下代码展示使用<code>DataLoader</code> 加载一个 <code>Dataset</code> 对象：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># 创建自定义数据集</span>
custom_dataset <span class="token operator">=</span> MyCustomDataset<span class="token punctuation">(</span>data<span class="token punctuation">,</span>csv_file<span class="token punctuation">,</span> root_dir<span class="token punctuation">)</span>

<span class="token comment"># 使用 DataLoader 加载数据</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>custom_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 迭代 DataLoader 并打印数据和标签</span>
<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-集成的数据集" tabindex="-1"><a class="header-anchor" href="#_2-集成的数据集"><span>2. 集成的数据集</span></a></h3><blockquote><p><code>torchvision.datasets</code></p></blockquote><p><code>torchvision</code> 是 PyTorch 中的一个视觉处理库，提供了数据集、图片变换等一系列图像处理操作。</p><p><code>torchvision.datasets</code> 中包含了一些常用的数据集，例如 <code>MNIST、CIFAR10、CIFAR100、ImageNet</code> 等。这些数据集通常是用于测试和比较不同模型的性能，同时也可以用于训练模型。</p><p>使用 <code>torchvision.datasets</code> 可以方便地下载、加载和预处理这些常用数据集。常用函数和参数如下：</p><ul><li><p><code>torchvision.datasets.MNIST</code></p><p>：加载 MNIST 数据集。</p><ul><li><code>root</code>：MNIST 数据集的根目录。</li><li><code>train</code>：选择是否加载训练集，默认为 True。</li><li><code>transform</code>：数据预处理函数，例如缩放、剪裁、随机切割等。</li><li><code>target_transform</code>：标签数据的预处理函数。</li><li><code>download</code>：是否下载 MNIST 数据集。如果数据已经下载过，则会直接读取本地数据。</li></ul></li><li><p><code>torchvision.datasets.CIFAR10</code>：加载 CIFAR10 数据集，参数和 MNIST 数据集类似。</p></li></ul><p>例如，以下代码给出了如何使用 <code>torchvision.datasets</code> 加载 MNIST 数据集并进行数据预处理：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> datasets 
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token comment"># 数据预处理，将输入数据放缩到 (0, 1) 范围内 </span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>    
    <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     
     transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 下载和加载数据集 </span>
train_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">&#39;data/MNIST&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">&#39;data/MNIST&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先定义了一个数据预处理函数 <code>transform</code>，将输入数据放缩到 (0, 1) 范围内。然后，我们使用 <code>datasets.MNIST</code> 来下载和加载 MNIST 数据集，其中传入了一些参数，例如 <code>root</code> 表示数据存放的根目录，<code>train</code> 表示加载训练数据集，<code>transform</code> 是数据预处理函数。</p><p>最后，我们得到了 <code>train_data</code> 和 <code>test_data</code> 两个数据集对象，分别用于模型的训练和测试。</p><h2 id="二、优化算法" tabindex="-1"><a class="header-anchor" href="#二、优化算法"><span>二、优化算法</span></a></h2><h3 id="_1-训练优化中的问题" tabindex="-1"><a class="header-anchor" href="#_1-训练优化中的问题"><span>1. 训练优化中的问题</span></a></h3><p><strong>局部最小值</strong></p><p>深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数<em>局部</em>最优，而不是<em>全局</em>最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。</p><figure><img src="`+p+'" alt="../_images/output_optimization-intro_70d214_51_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_51_0.svg</figcaption></figure><p><strong>鞍点</strong></p><p>除了局部最小值之外，鞍点是梯度消失的另一个原因。<em>鞍点</em>（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数f(x)=x^3。它的一阶和二阶导数在x=0时消失。这时优化可能会停止，尽管它不是最小值。</p><figure><img src="'+e+'" alt="../_images/output_optimization-intro_70d214_66_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_66_0.svg</figcaption></figure><p><strong>梯度消失</strong></p><p>假设我们想最小化函数$f(x)=tanh⁡(x)$，然后我们恰好从$x=4$开始。正如我们所看到的那样，f的梯度接近零。更具体地说，$f′(x)=1−tanh^2⁡(x)$，因此是$f′(4)=0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。</p><figure><img src="'+o+`" alt="../_images/output_optimization-intro_70d214_96_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_96_0.svg</figcaption></figure><h3 id="_2-优化算法" tabindex="-1"><a class="header-anchor" href="#_2-优化算法"><span>2. 优化算法</span></a></h3><p><strong>随机梯度下降</strong></p><blockquote><p><code>torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></p></blockquote><p><code>torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code> 是 PyTorch 中的一个优化器类，实现了随机梯度下降（SGD）算法。该算法是常用的优化算法之一，通过计算当前梯度和历史梯度的加权平均值来更新模型参数。</p><ul><li><code>params</code>：待优化的参数。</li><li><code>lr</code>：学习率，控制每次更新的步长大小。</li><li><code>momentum</code>：动量（Momentum）因子，加速优化过程，防止参数在更新方向改变时来回摆动。默认值为 0。</li><li><code>dampening</code>：动量的抑制因子，范围为 [0, 1]。默认值为 0。</li><li><code>weight_decay</code>：权重衰减因子，用于控制模型复杂度。默认为 0，表示不使用权重衰减。</li><li><code>nesterov</code>：是否使用 Nesterov 加速梯度，即使用当前位置的动量项进行梯度估计，可以提高优化效果。默认值为 False。</li></ul><p>例如，以下代码给出了如何使用 <code>torch.optim.SGD</code> 将模型的权重向量 w 进行随机梯度下降更新：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先定义了一个模型 <code>MyModel</code>，然后使用 <code>model.parameters()</code> 获取模型的参数，传入 <code>optim.SGD()</code> 函数中创建一个 <code>SGD</code> 优化器对象 <code>optimizer</code>。在模型的训练过程中，我们使用 <code>optimizer.zero_grad()</code> 来清空历史梯度，然后进行前向传播和反向传播，最后通过 <code>optimizer.step()</code> 来更新模型的参数。</p><h2 id="三、网络层结构" tabindex="-1"><a class="header-anchor" href="#三、网络层结构"><span>三、网络层结构</span></a></h2><h3 id="_1-展平层" tabindex="-1"><a class="header-anchor" href="#_1-展平层"><span>1. 展平层</span></a></h3><p><code>nn.Flatten()</code> 是 PyTorch 中的一个层（layer），用于将输入张量展平成一个向量，例如将一个大小为 <code>(n, c, h, w)</code> 的四维张量展平为一个大小为 <code>(n, c*h*w)</code> 的二维张量。</p><p>这个操作在神经网络中经常用到，通常出现在卷积和池化层后以便将其输出进入全连接层。具体来说，当卷积和池化层的输出为四维张量，而全连接层的输入为二维张量时，这时就需要使用 <code>nn.Flatten()</code> 来将张量展平为一个向量。</p><p>例如，以下代码演示了如何使用 <code>nn.Flatten()</code>：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> 
<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们定义了一个简单的 CNN 模型，其中包含了两个卷积层、两个 ReLU 激活层和两个 MaxPool 层。这些层的输出都是四维张量，因此我们需要在最后一个 MaxPool2d 层之后使用 <code>nn.Flatten()</code> 层将输出张量展平成二维张量，然后才能进入全连接层进行分类。</p><h3 id="_2-卷积层" tabindex="-1"><a class="header-anchor" href="#_2-卷积层"><span>2. 卷积层</span></a></h3><p><code>nn.Conv2d</code>是PyTorch中用于定义二维卷积层的函数。它是基于torch.nn.Module构造出的类，可以直接使用PyTorch提供的API进行调用，其主要参数包括：输入通道数，输出通道数，卷积核大小，步长、填充等。当定义了卷积层后，需要调用forward函数计算输出值。</p><p>例如，下面是一个使用nn.Conv2d定义卷积层的简单示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token comment"># 定义卷积层</span>
conv_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 假设输入一个大小为 [batch_size, 3, 28, 28] 的张量 x</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>

<span class="token comment"># 计算卷积层输出</span>
output <span class="token operator">=</span> conv_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment"># 输出结果的大小: [batch_size, 64, 28, 28]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个示例中，定义了一个输入通道数为3，输出通道数为64，卷积核大小为3x3，步长、填充均为1的卷积层，并将其应用到一个大小为[16, 3, 28, 28]的张量上，然后输出的结果大小是[16, 64, 28, 28]，其中16是batch_size的大小。</p><h3 id="_3-池化层" tabindex="-1"><a class="header-anchor" href="#_3-池化层"><span>3. 池化层</span></a></h3><blockquote><p><code>nn.MaxPool2d</code></p></blockquote><p><code>nn.MaxPool2d</code>是PyTorch神经网络库中提供的最大池化层。它可以在输入上执行2D最大池化操作，将输入划分为固定的池化区域，并在每个池化区域内找到最大值。具体而言，MaxPool2d的作用是将输入张量在每个池化窗口内取最大值，并将所有结果组合成一个张量，作为输出。</p><p>该函数有以下参数：</p><ul><li><code>kernel_size</code>: 池化窗口的大小（宽度，高度）。</li><li><code>stride</code>： 池化窗口的移动步长。</li><li><code>padding</code>：对高度和宽度的零填充数量</li><li><code>dilation</code>: 控制滤波器元素之间距离的参数。</li><li><code>return_indices</code>：返回由最大值组成的张量的位置的指标，而不是最大值本身。</li><li><code>ceil_mode</code>：是否输出图像大小向上取整</li></ul><p>下面是一个示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># Define a 2D max pooling layer</span>
maxpool_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># Apply max pooling to an input tensor</span>
input_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output_tensor <span class="token operator">=</span> maxpool_layer<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Input shape: </span><span class="token interpolation"><span class="token punctuation">{</span>input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Output shape: </span><span class="token interpolation"><span class="token punctuation">{</span>output_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码定义了一个kernel大小为2 * 2，步幅为2的2D最大池化层，并将该层应用于一个张量。输入张量的形状为（1，3，4，4），其中1表示批次大小，3表示通道数，4和4表示输入图像的宽度和高度。在这种情况下，MaxPool2d的输出张量形状为（1，3，2，2），因为窗口中的4个池化区域中的每个都有一个输出值。在这个示例中，不使用其他参数，因此默认参数被用到。</p><hr><blockquote><p><code>nn.AdaptiveAvgPool2d</code></p></blockquote><p><code>nn.AdaptiveAvgPool2d</code>是PyTorch神经网络库中提供的自适应平均池化层。它对输入进行二维平均池化，使输出具有特定形状。它可以自适应地考虑更大或更小的输入张量，并根据指定的输出大小调整窗口大小和步幅，以使输出具有期望的大小。</p><p>这个函数有一个必须的参数输出大小（output_size），我们希望输出的张量大小为（batch_size，channel，output_size，output_size）</p><p><code>nn.AdaptiveAvgPool2d</code>的输出张量形状为指定大小的输出，无论输入张量的尺寸如何，它会自动调整窗口大小和步幅。</p><p>下面是一个示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># Define a 2D adaptive average pooling layer</span>
adaptive_avg_pool_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># Apply adaptive average pooling to an input tensor</span>
input_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output_tensor <span class="token operator">=</span> adaptive_avg_pool_layer<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Input shape: </span><span class="token interpolation"><span class="token punctuation">{</span>input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Output shape: </span><span class="token interpolation"><span class="token punctuation">{</span>output_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码定义了一个2D自适应平均池化层，并将其应用于一个输入张量。输入张量的形状为（1，3，4，4），其中1表示批次大小，3表示通道数，4和4表示输入图像的宽度和高度。 在这个示例中，输出大小为（2,2），因此输出张量的形状为（1，3，2，2）。</p><h3 id="_4-dropout" tabindex="-1"><a class="header-anchor" href="#_4-dropout"><span>4. Dropout</span></a></h3><p><code>dropout</code>是一种常用的正则化方法用于减少神经网络的过拟合风险。 在运行时期间，<code>dropout</code>以一定的概率丢弃（设置于0）输入单元或输出单元的权重，然后从训练数据集的小批量数据中随机选择新的输入单元或输出单元的权重。 过拟合风险较大的神经元将会有较大的概率被丢弃，保留较小的概率，以减轻神经元之间的复杂协同关系。</p><p>在深度学习网络训练过程中，<code>dropout</code>层一般加在全连接层之后，可以通过<code>nn.Dropout(p = probability)</code>直接实现。p为丢弃的概率，通常取0.2~0.5之间的值。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_5-batchnorm" tabindex="-1"><a class="header-anchor" href="#_5-batchnorm"><span>5. BatchNorm</span></a></h3><p><code>nn.BatchNorm2d</code>是PyTorch中的一个批量归一化（Batch Normalization）操作类。它会对输入进行标准化处理，让前一层的输出其均值为0，方差为1，并通过可学习的缩放和偏移参数，使模型能够恢复任意的变换。</p><p>该类的主要参数包括：</p><ul><li><code>num_features</code>：输入特征的通道数。</li><li><code>eps</code>：为了避免分母为零的情况，在分母上加上一个很小的数 eps。</li><li><code>momentum</code>：用于计算移动平均值的动量系数，用于更新求得的均值和方差。</li><li><code>affine</code>: bool类型，默认值为True，表示是否启用可学习的缩放和平移参数。</li><li><code>track_running_stats</code>: bool类型，默认值为True，表示是否使用运行统计信息（如平均值和标准差）来标准化。</li></ul><p>下面是一个简单的使用实例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 初始化一个 BatchNorm2d</span>
bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> bn<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>该例子中，<code>num_features</code>被设置为3，表示输入特征的通道数为3。输入的张量<code>input</code>的形状为(2, 3, 4, 4)，表示batch_size为2，通道数为3，每个特征图的大小为(4, 4)。执行<code>bn(input)</code>后，返回的输出张量<code>output</code>具有相同的形状。</p><p>在训练过程中，<code>BatchNorm2d</code> 对接收到的每个 <code>feature map</code> 分别执行以下操作：</p><ol><li>计算每个 feature map 的均值和方差，并通过移动平均的方式整合到全局均值和方差中。</li><li>根据上述的均值和方差，对 feature map 进行标准化处理。</li><li>通过可学习的缩放和偏移参数进行线性变换。</li><li>返回标准化后的 feature map。</li></ol><p>需要注意的是，在测试模式下，BatchNorm2d 的运作方式与训练模式不同，其前向传播不会使用 mini-batch 的均值和方差，而是使用全局的均值和方差（在训练过程中通过动量方式累积得到的均值和方差）来进行标准化处理。</p><h3 id="_6-relu" tabindex="-1"><a class="header-anchor" href="#_6-relu"><span>6. Relu</span></a></h3><p><code>nn.ReLU</code>是对输入的数据进行修正线性单元(Rectified Linear Units, ReLU)操作的类。ReLU是深度学习中常用的激活函数，它可以通过将负值变为0来增强线性神经网络的非线性性。其代数式为$f(x) = max(0,x)$，即当输入x大于0时$f(x)$等于$x$，否则$f(x)$等于0。</p><p><code>nn.ReLU</code>的功能很简单，它接收输入张量并将其作用于ReLU激活函数，输出ReLU激活后的张量。在pytorch中使用<code>nn.ReLU</code>的代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> relu<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中<code>input</code>是输入张量，<code>output</code>是经过ReLU激活后的输出张量。</p><blockquote><p><code>nn.ReLU(inplace=True)</code></p></blockquote><p><code>nn.ReLU(inplace=True)</code>与<code>nn.ReLU()</code>的区别在于，它将原地修改输入张量而不是创建一个新的张量作为输出，并返回已修改的输入张量。</p><p>这意味着，使用<code>nn.ReLU(inplace=True)</code>会省略内存分配和释放，因为不会产生新的内存分配和释放操作。这在处理大量数据时，可以有效地减少内存开销，因为每次内存分配和释放都需要消耗大量的时间和资源。同时，使用inplace的操作还可以提高计算性能，因为不需要将数据从一个位置复制到另一个位置。</p><p>需要注意的是，使用<code>inplace=True</code>有时也可能会导致不可预测的结果或者梯度计算的错误。因此，在使用时需要谨慎，并根据具体情况判断是否需要开启inplace模式。</p><p><code>nn.ReLU</code>可能存在的问题有两个：</p><ol><li>梯度消失或爆炸：当输入很大或很小的时候，ReLU的导数会变成0或无穷大，从而导致梯度消失或爆炸的问题。</li><li>神经元死亡：如果训练期间一个ReLU神经元被更新成一直输出0的状态，那么在以后的训练过程中该神经元的参数将一直保持不变，也就是被&quot;死亡&quot;了。这种情况的发生通常与学习率过高有关，可以通过减小学习率或使用其他激活函数来缓解。</li></ol><h3 id="_7-反卷积层" tabindex="-1"><a class="header-anchor" href="#_7-反卷积层"><span>7. 反卷积层</span></a></h3><p><code>nn.ConvTranspose2d</code> 是 PyTorch 中的一个卷积转置层，用于进行反卷积（Deconvolution）操作。它的作用是将一个低维的特征图映射到一个高维空间中，可以用于图像分割、语义分割、超分辨率等任务。</p><p>在具体实现中，<code>nn.ConvTranspose2d</code> 与 <code>nn.Conv2d</code> 的实现方式十分相似，都是通过卷积核与输入的特征图进行卷积操作，这里不再赘述。主要的区别在于反卷积操作是将卷积核中的值进行上采样之后进行卷积，从而实现特征图的上采样，对应于卷积操作中的下采样。</p><p>具体来说， <code>nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)</code> 的参数含义介绍如下：</p><ul><li><code>in_channels</code> : 输入特征图的通道数。</li><li><code>out_channels</code> : 输出特征图的通道数。</li><li><code>kernel_size</code> : 卷积核的大小。</li><li><code>stride</code> : 步长，默认值为 1。</li><li><code>padding</code> : 填充大小，默认值为 0。</li><li><code>output_padding</code> : 输出特征图填充大小，默认值为 0。</li><li><code>groups</code> : 组数，默认值为 1。一个 ConvTranspose2d 连接了多个输入通道组和多个输出通道组。</li><li><code>bias</code> : 是否使用偏置，默认为 <code>True</code>。</li><li><code>dilation</code> : 卷积核的膨胀率，默认为 1。</li></ul><p>使用方法如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

conv_trans <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 在输入特征图上进行反卷积</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> conv_trans<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输出形状为 [1, 64, 25, 25]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这里，我们首先创建了一个反卷积层 <code>conv_trans</code>，将输入通道数设为 32，输出通道数设为 64，卷积核大小为 (3, 3)。接着，我们使用 PyTorch 中的 <code>torch.randn()</code> 函数创建了一个大小为 (1, 32, 12, 12) 的输入特征图，其中 1 表示 batch_size，32 表示输入通道数，12 表示特征图高和宽。最后，我们对输入图像做了反卷积操作，输出的特征图大小为 (1, 64, 25, 25)，其中特征图高和宽分别为 25，可以根据卷积转置的计算公式推导来进行理解。</p><h2 id="四、参数访问" tabindex="-1"><a class="header-anchor" href="#四、参数访问"><span>四、参数访问</span></a></h2><h3 id="_1-named-parameters-或parameters" tabindex="-1"><a class="header-anchor" href="#_1-named-parameters-或parameters"><span>1. <code>named_parameters()</code>或<code>parameters()</code></span></a></h3><p>使用model.named_parameters()或model.parameters()方法，返回一个生成器，遍历每个参数并返回对应的参数张量及名称，代码示例如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># named_parameters()方法获取网络层参数及名称</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><code>model.named_parameters</code>访问的是所有层的参数，要想访问具体某一层的参数，只需要<code>model.(层name).named_parameters</code>.</p></blockquote><h3 id="_2-model-layer-weight" tabindex="-1"><a class="header-anchor" href="#_2-model-layer-weight"><span>2. <code>model.layer.weight</code></span></a></h3><p>使用<code>model.layer.weight</code>和<code>model.layer.bias</code>直接获取某一层的权重和偏置，示例代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 直接访问module的成员变量获取网络层参数</span>
weight <span class="token operator">=</span> model<span class="token punctuation">.</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data
bias <span class="token operator">=</span> model<span class="token punctuation">.</span>layer<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data
<span class="token keyword">print</span><span class="token punctuation">(</span>weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>直接使用<code>model.layer.weight</code>，返回值除了值外，还包括梯度等额外信息。</li></ul><h3 id="_3-state-dict" tabindex="-1"><a class="header-anchor" href="#_3-state-dict"><span>3. <code>state_dict</code></span></a></h3><p>使用model.state_dict()方法获取网络层的参数字典，该字典包含每个层的名称和参数，例如<code>model.state_dict()[&#39;layer.weight&#39;]</code>，示例代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># state_dict方法获取网络层参数</span>
params_dict <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>
weight <span class="token operator">=</span> params_dict<span class="token punctuation">[</span><span class="token string">&#39;layer.weight&#39;</span><span class="token punctuation">]</span>
bias <span class="token operator">=</span> params_dict<span class="token punctuation">[</span><span class="token string">&#39;layer.bias&#39;</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>下面是<code>state_dict()</code>返回的结果样例</p></blockquote><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">&#39;weight&#39;</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3016</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1901</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1991</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1220</span><span class="token punctuation">,</span>  <span class="token number">0.1121</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1424</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3060</span><span class="token punctuation">,</span>  <span class="token number">0.3400</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&#39;bias&#39;</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0291</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="五、保存和读取模型参数" tabindex="-1"><a class="header-anchor" href="#五、保存和读取模型参数"><span>五、保存和读取模型参数</span></a></h2><h3 id="_1-保存和读取" tabindex="-1"><a class="header-anchor" href="#_1-保存和读取"><span>1. 保存和读取</span></a></h3><p>PyTorch 中，我们可以使用 <code>state_dict</code> 来保存和读取模型的参数。<code>state_dict</code> 是一个 Python 字典对象，它将每个层映射到其参数张量。可以将 <code>state_dict</code> 存储为文件以备以后使用，也可以将其作为 Python 对象传递给其他 PyTorch 模型。</p><p>下面是一个简单的示例，说明如何保存和读取模型参数：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存模型参数</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&#39;model_params.pth&#39;</span><span class="token punctuation">)</span>

<span class="token comment"># 加载模型参数</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;model_params.pth&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这个示例中，我们首先定义了一个简单的模型 <code>MyModel</code>，它包含一个线性层。接下来，我们通过调用 <code>torch.save</code> 函数，将模型的 <code>state_dict</code> 保存到名为 <code>&#39;model_params.pth&#39;</code> 的文件中。</p><p>最后，我们使用 <code>torch.load</code> 函数加载保存的 <code>state_dict</code>。然后调用 <code>model.load_state_dict</code> 将加载的 <code>state_dict</code> 传递给模型。这将更新模型的参数。</p><p>PyTorch 也支持 <code>save</code> 和 <code>load</code> 函数，这些函数将整个模型保存到文件中，并支持在 Python 和 C++ 中加载模型。这些函数在处理多个对象时更具有灵活性，但 <code>state_dict</code> 更适用于保存和加载模型参数。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>mydict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;x&#39;</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">&#39;y&#39;</span><span class="token punctuation">:</span> y<span class="token punctuation">}</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>mydict<span class="token punctuation">,</span> <span class="token string">&#39;mydict&#39;</span><span class="token punctuation">)</span>
mydict2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;mydict&#39;</span><span class="token punctuation">)</span>
mydict2
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;x&#39;</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&#39;y&#39;</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_2-查看数据结构" tabindex="-1"><a class="header-anchor" href="#_2-查看数据结构"><span>2. 查看数据结构</span></a></h3><p>使用torch.save()函数保存的模型权重参数文件默认是以Python的pickle格式进行序列化的，包括datasets中数据集默认也是以这种方式进行的存储，这种文件直接使用文本打开会显示乱码，我们可以利用pickle对文件进行反序列化，以查看其数据结构：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> pickle
filename <span class="token operator">=</span> <span class="token string">&#39;../cifar10/cifar-10-batches-py/test_batch&#39;</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>filename<span class="token punctuation">,</span><span class="token string">&#39;rb&#39;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    dataset <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">&#39;bytes&#39;</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># out: &lt;class &#39;dict&#39;&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="六、gpu相关" tabindex="-1"><a class="header-anchor" href="#六、gpu相关"><span>六、GPU相关</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>nvidia-smi
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ul><li>使用<code>nvidia-smi</code>命令来查看显卡信息</li></ul><figure><img src="`+c+`" alt="image-20230329155746146" tabindex="0" loading="lazy"><figcaption>image-20230329155746146</figcaption></figure><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cpu&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># cuda和cuda:0等价</span>
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda:1&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span> 

<span class="token comment"># 常用</span>
device <span class="token operator">=</span> <span class="token string">&#39;cuda&#39;</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">&#39;cpu&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-将模型参数放在gpu" tabindex="-1"><a class="header-anchor" href="#_1-将模型参数放在gpu"><span>1. 将模型参数放在gpu</span></a></h3><ol><li><p>在创建的时候指定device</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用cuda()复制</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>Z <span class="token operator">=</span> X<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 参数为cuda的编号</span>
Z
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用to()指定</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ol><h3 id="_2-切换模型模式" tabindex="-1"><a class="header-anchor" href="#_2-切换模型模式"><span>2. 切换模型模式</span></a></h3><ul><li>切换：<code>model.train()和model.eval()</code></li><li>判断：<code>torch.is_grad_enabled()</code><ul><li>检查当前PyTorch计算图是否启用梯度计算。它返回一个布尔值，如果梯度计算启用，则返回True，否则返回False。</li></ul></li></ul><h2 id="七、注意力机制" tabindex="-1"><a class="header-anchor" href="#七、注意力机制"><span>七、注意力机制</span></a></h2><p>灵长类动物的视觉系统接受了大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>在注意力机制中，会使用到三个重要的概念：查询(Query)、键(Keys)和值(Value)。</p><ul><li><p>查询(Query)：代表我们要对输入中的哪一部分进行关注和处理，通常是在计算注意力权重时所依据的向量。它是通过上一步的输出来计算的，具体的计算方式取决于注意力机制的不同类型。</p></li><li><p>键(Keys)：它是用来表示输入序列中各个位置的特征的向量，一般是通过对输入进行变换（如全连接层或卷积操作）获得。</p></li><li><p>值(Value)：它是输入序列中的各个位置所对应的实际数值（训练时的输出y），是未经过变换的原始输入。</p></li></ul><p>在注意力机制中，我们做的实际上是以下3件事：</p><ul><li>首先将查询向量与键向量计算余弦相似度或欧式距离（评分函数） <ul><li>越小代表键靠近自己想查询的值，随之计算出的权重越大</li></ul></li><li>进行 softmax 激活后得到注意力权重</li><li>最后将注意力权重与值向量进行加权求和</li></ul><p>这个操作的目的是对不同的输入位置赋予不同的权重，使得在对序列进行处理时，对最相关和最重要的部分给予更大的关注和权重，从而提高序列的表示能力。</p><figure><img src="`+l+`" alt="../_images/attention-output.svg" tabindex="0" loading="lazy"><figcaption>../_images/attention-output.svg</figcaption></figure><h2 id="八、一些通用函数" tabindex="-1"><a class="header-anchor" href="#八、一些通用函数"><span>八、一些通用函数</span></a></h2><h3 id="_1-训练函数" tabindex="-1"><a class="header-anchor" href="#_1-训练函数"><span>1. 训练函数</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">train_ch6</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 初始化权重</span>
    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear <span class="token keyword">or</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;training on:&#39;</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token comment"># 优化器</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    <span class="token comment"># 损失函数</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">&#39;epoch&#39;</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>
                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&#39;train loss&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;train acc&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;test acc&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    timer<span class="token punctuation">,</span> num_batches <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span> <span class="token comment">#迭代次数</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 训练损失之和，训练准确率之和，样本数</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
            timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_l <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            train_acc <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_batches <span class="token operator">//</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> i <span class="token operator">==</span> num_batches <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_batches<span class="token punctuation">,</span>
                             <span class="token punctuation">(</span>train_l<span class="token punctuation">,</span> train_acc<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> evaluate_accuracy_gpu<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>
        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_l<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, train acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, &#39;</span></span>
          <span class="token string-interpolation"><span class="token string">f&#39;test acc: </span><span class="token interpolation"><span class="token punctuation">{</span>test_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;</span><span class="token interpolation"><span class="token punctuation">{</span>metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> num_epochs <span class="token operator">/</span> timer<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string"> examples/sec &#39;</span></span>
          <span class="token string-interpolation"><span class="token string">f&#39;on </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,160),r=[u];function d(k,m){return s(),a("div",null,r)}const h=n(i,[["render",d],["__file","torch_api.html.vue"]]),g=JSON.parse('{"path":"/paper/code/torch_api.html","title":"pytorch基础","lang":"zh-CN","frontmatter":{"title":"pytorch基础","icon":"pytorch","description":"一、导入数据集 1. 处理数据集 torch.utils.data.TensorDataset(*tensors) 这是 PyTorch 中的一个数据集类，用于针对张量数据创建数据集，即将张量表示的数据封装为数据集对象，以便于之后用于训练模型。 该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签...","head":[["meta",{"property":"og:url","content":"https://github.com/dcblog/paper/code/torch_api.html"}],["meta",{"property":"og:site_name","content":"dcBlog"}],["meta",{"property":"og:title","content":"pytorch基础"}],["meta",{"property":"og:description","content":"一、导入数据集 1. 处理数据集 torch.utils.data.TensorDataset(*tensors) 这是 PyTorch 中的一个数据集类，用于针对张量数据创建数据集，即将张量表示的数据封装为数据集对象，以便于之后用于训练模型。 该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-03-19T13:13:32.000Z"}],["meta",{"property":"article:author","content":"Dachao"}],["meta",{"property":"article:modified_time","content":"2024-03-19T13:13:32.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"pytorch基础\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2024-03-19T13:13:32.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Dachao\\",\\"email\\":\\"wyc168hard@163.com\\"}]}"]]},"headers":[{"level":2,"title":"一、导入数据集","slug":"一、导入数据集","link":"#一、导入数据集","children":[{"level":3,"title":"1. 处理数据集","slug":"_1-处理数据集","link":"#_1-处理数据集","children":[]},{"level":3,"title":"2. 集成的数据集","slug":"_2-集成的数据集","link":"#_2-集成的数据集","children":[]}]},{"level":2,"title":"二、优化算法","slug":"二、优化算法","link":"#二、优化算法","children":[{"level":3,"title":"1. 训练优化中的问题","slug":"_1-训练优化中的问题","link":"#_1-训练优化中的问题","children":[]},{"level":3,"title":"2. 优化算法","slug":"_2-优化算法","link":"#_2-优化算法","children":[]}]},{"level":2,"title":"三、网络层结构","slug":"三、网络层结构","link":"#三、网络层结构","children":[{"level":3,"title":"1. 展平层","slug":"_1-展平层","link":"#_1-展平层","children":[]},{"level":3,"title":"2. 卷积层","slug":"_2-卷积层","link":"#_2-卷积层","children":[]},{"level":3,"title":"3. 池化层","slug":"_3-池化层","link":"#_3-池化层","children":[]},{"level":3,"title":"4. Dropout","slug":"_4-dropout","link":"#_4-dropout","children":[]},{"level":3,"title":"5. BatchNorm","slug":"_5-batchnorm","link":"#_5-batchnorm","children":[]},{"level":3,"title":"6.  Relu","slug":"_6-relu","link":"#_6-relu","children":[]},{"level":3,"title":"7. 反卷积层","slug":"_7-反卷积层","link":"#_7-反卷积层","children":[]}]},{"level":2,"title":"四、参数访问","slug":"四、参数访问","link":"#四、参数访问","children":[{"level":3,"title":"1. named_parameters()或parameters()","slug":"_1-named-parameters-或parameters","link":"#_1-named-parameters-或parameters","children":[]},{"level":3,"title":"2.  model.layer.weight","slug":"_2-model-layer-weight","link":"#_2-model-layer-weight","children":[]},{"level":3,"title":"3. state_dict","slug":"_3-state-dict","link":"#_3-state-dict","children":[]}]},{"level":2,"title":"五、保存和读取模型参数","slug":"五、保存和读取模型参数","link":"#五、保存和读取模型参数","children":[{"level":3,"title":"1. 保存和读取","slug":"_1-保存和读取","link":"#_1-保存和读取","children":[]},{"level":3,"title":"2. 查看数据结构","slug":"_2-查看数据结构","link":"#_2-查看数据结构","children":[]}]},{"level":2,"title":"六、GPU相关","slug":"六、gpu相关","link":"#六、gpu相关","children":[{"level":3,"title":"1. 将模型参数放在gpu","slug":"_1-将模型参数放在gpu","link":"#_1-将模型参数放在gpu","children":[]},{"level":3,"title":"2. 切换模型模式","slug":"_2-切换模型模式","link":"#_2-切换模型模式","children":[]}]},{"level":2,"title":"七、注意力机制","slug":"七、注意力机制","link":"#七、注意力机制","children":[]},{"level":2,"title":"八、一些通用函数","slug":"八、一些通用函数","link":"#八、一些通用函数","children":[{"level":3,"title":"1. 训练函数","slug":"_1-训练函数","link":"#_1-训练函数","children":[]}]}],"git":{"createdTime":1710854012000,"updatedTime":1710854012000,"contributors":[{"name":"dachao","email":"1114686398@qq.com","commits":1}]},"readingTime":{"minutes":23.9,"words":7170},"filePathRelative":"paper/code/torch_api.md","localizedDate":"2024年3月19日","excerpt":"<h2>一、导入数据集</h2>\\n<h3>1. 处理数据集</h3>\\n<blockquote>\\n<p><code>torch.utils.data.TensorDataset(*tensors)</code></p>\\n</blockquote>\\n<p>这是 PyTorch 中的一个数据集类，用于针对<strong>张量数据</strong>创建数据集，即<strong style=\\"color:red\\">将张量表示的数据封装为数据集对象</strong>，以便于之后用于训练模型。</p>\\n<p>该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签，则需要使用两个张量来描述数据集，一个张量表示图像数据，另一个张量表示标签数据。</p>","autoDesc":true}');export{h as comp,g as data};

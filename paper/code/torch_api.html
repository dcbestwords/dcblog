<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.8" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.31" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://github.com/dcblog/paper/code/torch_api.html"><meta property="og:site_name" content="dcBlog"><meta property="og:title" content="pytorch基础"><meta property="og:description" content="一、导入数据集 1. 处理数据集 torch.utils.data.TensorDataset(*tensors) 这是 PyTorch 中的一个数据集类，用于针对张量数据创建数据集，即将张量表示的数据封装为数据集对象，以便于之后用于训练模型。 该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2024-03-19T13:13:32.000Z"><meta property="article:author" content="Dachao"><meta property="article:modified_time" content="2024-03-19T13:13:32.000Z"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"pytorch基础","image":[""],"dateModified":"2024-03-19T13:13:32.000Z","author":[{"@type":"Person","name":"Dachao","email":"wyc168hard@163.com"}]}</script><link rel="icon" href="/dcblog/favicon.ico"><link rel="icon" href="/dcblog/assets/icon/chrome-mask-512.png" type="image/png" sizes="512x512"><link rel="icon" href="/dcblog/assets/icon/chrome-mask-192.png" type="image/png" sizes="192x192"><link rel="icon" href="/dcblog/assets/icon/chrome-512.png" type="image/png" sizes="512x512"><link rel="icon" href="/dcblog/assets/icon/chrome-192.png" type="image/png" sizes="192x192"><link rel="manifest" href="/dcblog/manifest.webmanifest" crossorigin="use-credentials"><meta name="theme-color" content="#46bd87"><link rel="apple-touch-icon" href="/dcblog/assets/icon/apple-icon-152.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="msapplication-TileImage" content="/dcblog/assets/icon/ms-icon-144.png"><meta name="msapplication-TileColor" content="#ffffff"><title>pytorch基础 | dcBlog</title><meta name="description" content="一、导入数据集 1. 处理数据集 torch.utils.data.TensorDataset(*tensors) 这是 PyTorch 中的一个数据集类，用于针对张量数据创建数据集，即将张量表示的数据封装为数据集对象，以便于之后用于训练模型。 该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签...">
    <link rel="preload" href="/dcblog/assets/style-2JG01pdU.css" as="style"><link rel="stylesheet" href="/dcblog/assets/style-2JG01pdU.css">
    <link rel="modulepreload" href="/dcblog/assets/app-BPd7oWPf.js"><link rel="modulepreload" href="/dcblog/assets/torch_api.html-DAcY8RW5.js"><link rel="modulepreload" href="/dcblog/assets/plugin-vue_export-helper-DlAUqK2U.js">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="route-link vp-brand" href="/dcblog/"><img class="vp-nav-logo" src="/dcblog/logo.png" alt><!----><span class="vp-site-name hide-in-pad">dcBlog</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/" aria-label="主页"><span class="font-icon icon iconfont icon-home" style=""></span>主页<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/basic_language/" aria-label="语言基础"><span class="font-icon icon iconfont icon-code" style=""></span>语言基础<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/source_code/" aria-label="源码解析"><span class="font-icon icon iconfont icon-read" style=""></span>源码解析<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/Server/" aria-label="服务端"><span class="font-icon icon iconfont icon-back-stage" style=""></span>服务端<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/Efficiency/" aria-label="效率协作"><span class="font-icon icon iconfont icon-group" style=""></span>效率协作<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/browser_internet/" aria-label="浏览器通信"><span class="font-icon icon iconfont icon-network" style=""></span>浏览器通信<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/algorithm/" aria-label="算法"><span class="font-icon icon iconfont icon-edit" style=""></span>算法<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/Project_50/" aria-label="页面练习"><span class="font-icon icon iconfont icon-page" style=""></span>页面练习<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link active" href="/dcblog/paper/" aria-label="论文课题"><span class="font-icon icon iconfont icon-study" style=""></span>论文课题<!----></a></div><div class="nav-item hide-in-mobile"><a class="route-link nav-link" href="/dcblog/reference/" aria-label="文档教程"><span class="font-icon icon iconfont icon-article" style=""></span>文档教程<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/dcbestwords/dcBlog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><!--[--><button type="button" class="search-pro-button" aria-label="搜索"><svg xmlns="http://www.w3.org/2000/svg" class="icon search-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="search icon"><path d="M192 480a256 256 0 1 1 512 0 256 256 0 0 1-512 0m631.776 362.496-143.2-143.168A318.464 318.464 0 0 0 768 480c0-176.736-143.264-320-320-320S128 303.264 128 480s143.264 320 320 320a318.016 318.016 0 0 0 184.16-58.592l146.336 146.368c12.512 12.48 32.768 12.48 45.28 0 12.48-12.512 12.48-32.768 0-45.28"></path></svg><div class="search-pro-placeholder">搜索</div><div class="search-pro-key-hints"><kbd class="search-pro-key">Ctrl</kbd><kbd class="search-pro-key">K</kbd></div></button><!--]--><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable active"><span class="font-icon icon iconfont icon-pytorch" style=""></span><a class="route-link nav-link active vp-sidebar-title" href="/dcblog/paper/code/" aria-label="pytorch"><!---->pytorch<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link active vp-sidebar-link vp-sidebar-page active" href="/dcblog/paper/code/torch_api.html" aria-label="pytorch基础"><span class="font-icon icon iconfont icon-pytorch" style=""></span>pytorch基础<!----></a></li></ul></section></li><li><section class="vp-sidebar-group"><p class="vp-sidebar-header clickable"><span class="font-icon icon iconfont icon-study" style=""></span><a class="route-link nav-link vp-sidebar-title" href="/dcblog/paper/paper/" aria-label="论文笔记"><!---->论文笔记<!----></a><!----></p><ul class="vp-sidebar-links"><li><a class="route-link nav-link vp-sidebar-link vp-sidebar-page" href="/dcblog/paper/paper/test.html" aria-label="论文笔记"><!---->论文笔记<!----></a></li></ul></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><span class="font-icon icon iconfont icon-pytorch" style=""></span>pytorch基础</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Dachao</span></span><span property="author" content="Dachao"></span></span><!----><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span><!----></span><meta property="datePublished" content="2024-03-19T13:13:32.000Z"></span><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 24 分钟</span><meta property="timeRequired" content="PT24M"></span><!----><!----></div><hr></div><div class="vp-toc-placeholder"><aside id="toc"><!--[--><!----><!--]--><div class="vp-toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button><div class="arrow end"></div></div><div class="vp-toc-wrapper"><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#一、导入数据集">一、导入数据集</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-处理数据集">1. 处理数据集</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-集成的数据集">2. 集成的数据集</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#二、优化算法">二、优化算法</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-训练优化中的问题">1. 训练优化中的问题</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-优化算法">2. 优化算法</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#三、网络层结构">三、网络层结构</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-展平层">1. 展平层</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-卷积层">2. 卷积层</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_3-池化层">3. 池化层</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_4-dropout">4. Dropout</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_5-batchnorm">5. BatchNorm</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_6-relu">6.  Relu</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_7-反卷积层">7. 反卷积层</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#四、参数访问">四、参数访问</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-named-parameters-或parameters">1. named_parameters()或parameters()</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-model-layer-weight">2.  model.layer.weight</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_3-state-dict">3. state_dict</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#五、保存和读取模型参数">五、保存和读取模型参数</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-保存和读取">1. 保存和读取</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-查看数据结构">2. 查看数据结构</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#六、gpu相关">六、GPU相关</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-将模型参数放在gpu">1. 将模型参数放在gpu</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_2-切换模型模式">2. 切换模型模式</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#七、注意力机制">七、注意力机制</a></li><!----><!--]--><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level2" href="/dcblog/#八、一些通用函数">八、一些通用函数</a></li><li><ul class="vp-toc-list"><!--[--><li class="vp-toc-item"><a class="route-link vp-toc-link level3" href="/dcblog/#_1-训练函数">1. 训练函数</a></li><!----><!--]--></ul></li><!--]--></ul><div class="vp-toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h2 id="一、导入数据集" tabindex="-1"><a class="header-anchor" href="#一、导入数据集"><span>一、导入数据集</span></a></h2><h3 id="_1-处理数据集" tabindex="-1"><a class="header-anchor" href="#_1-处理数据集"><span>1. 处理数据集</span></a></h3><blockquote><p><code>torch.utils.data.TensorDataset(*tensors)</code></p></blockquote><p>这是 PyTorch 中的一个数据集类，用于针对<strong>张量数据</strong>创建数据集，即<strong style="color:red;">将张量表示的数据封装为数据集对象</strong>，以便于之后用于训练模型。</p><p>该类接受一个或多个张量作为参数，其中每个张量表示数据集中的一个特征。例如，如果一个数据集包含图像和相应的标签，则需要使用两个张量来描述数据集，一个张量表示图像数据，另一个张量表示标签数据。</p><p>该类的初始化函数将所有参数合并为一个张量元组，即 <code>(*tensors)</code>，同时还可以定义一些额外的参数，例如 <code>transform=None</code> 用于对数据进行转换，<code>target_transform=None</code> 用于对标签进行转换。</p><p>例如，以下代码创建了一个 <code>TensorDataset</code> 对象，包含数据 <code>X</code> 和标签 <code>y</code>：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们创建了一个大小为 <code>(100, 10)</code> 的张量 <code>X</code>，和一个大小为 <code>(100,)</code> 的张量 <code>y</code>，分别代表输入数据和标签。然后，我们创建了一个 <code>TensorDataset</code> 对象 <code>ds</code>，将 <code>X</code> 和 <code>y</code> 作为参数传入。这样，我们就将数据 <code>X</code> 与标签 <code>y</code> 封装为了一个 PyTorch 的数据集对象，即 <code>ds</code>。</p><blockquote><p><code>torch.utils.data.Dataset</code></p></blockquote><p>这是一个抽象类，用作自定义数据集的基础。如果要<strong>自定义数据集</strong>，需要继承它并实现其中的两个方法：<code>len()</code> 和 <code>getItem()</code>。</p><p><code>len()</code> 方法返回数据集中的样本数量，<code>getitem()</code> 方法传递样本索引并返回样本及其标签（如果不需要标签，则可以选择省略）。在<code>__init__</code>方法中，我们可以将数据集读取到内存中，可以使用Pandas库等来读取CSV格式的数据，也可以通过自己编写函数读取图像数据。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image

<span class="token keyword">class</span> <span class="token class-name">MyCustomDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_file<span class="token punctuation">,</span> root_dir<span class="token punctuation">,</span> transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">&quot;&quot;&quot;
        csv_file (string): 文件路径，包括标签和图像信息。
        root_dir (string): 图像所在的目录。
        transform (callable, optional): 可选的图片转换(自定义或PyTorch中的)操作来增强数据。
        &quot;&quot;&quot;</span>
        self<span class="token punctuation">.</span>annotations <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>csv_file<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>root_dir <span class="token operator">=</span> root_dir
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>annotations<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        img_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>root_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>annotations<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        image <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
        label <span class="token operator">=</span> self<span class="token punctuation">.</span>annotations<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>
            image <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token punctuation">(</span>image<span class="token punctuation">,</span> label<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>在训练模型时，我们可以通过 <code>DataLoader</code> 类来使用 <code>TensorDataset</code> 或者<code>Dataset</code>对象，从而实现批量加载数据，并且可以进行数据增强和多线程加载等操作。</li></ul><blockquote><p><code>torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False)</code></p></blockquote><p>这是 PyTorch 中用于<strong>数据加载</strong>的工具类，用于<strong style="color:red;">在训练过程中批量加载数据，进行数据增强、数据划分和多线程等操作</strong>。</p><ul><li><code>dataset</code>: 加载数据所在的数据集。</li><li><code>batch_size</code>: 批处理大小。</li><li><code>shuffle</code>: 是否打乱数据。</li><li><code>sampler</code>: 样本抽样方式，可以自定义。</li><li><code>num_workers</code>: 线程的数量，0表示不使用多线程。</li><li><code>collate_fn</code>: 自定义的 batch 操作。</li><li><code>pin_memory</code>: 是否将数据保存到 CUDA 的固定内存上，提高 GPU 效率。</li><li><code>drop_last</code>: 如果 dataset 中数据总数不能被 batch_size 整除，则 drop_last 如果为 True，则将多余的数据删除；否则放到最后一个 batch 里去。</li></ul><p>例如，以下代码展示了如何使用 <code>DataLoader</code> 加载一个 <code>TensorDataset</code> 对象：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch 
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span> DataLoader

X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> 
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> 

loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>    
    <span class="token comment"># 进行训练或推理的代码    pass </span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先创建了一个大小为 <code>(100, 10)</code> 的数据张量 <code>X</code> 和大小为 <code>(100,)</code> 的标签张量 <code>y</code>，然后使用 <code>TensorDataset</code> 将这两个张量封装成一个数据集 <code>ds</code>。</p><p>接着，我们使用 <code>DataLoader</code> 对象 <code>loader</code> 加载数据集，批处理大小为 10，shuffle 为 True 表示每个 epoch 都会对数据进行随机打乱。在训练模型时，我们可以通过 <code>enumerate</code> 函数对 <code>loader</code> 进行迭代，通过 <code>(data, target)</code> 获取每个批次的输入数据和标签数据，进行模型的训练和推理。</p><p>以下代码展示使用<code>DataLoader</code> 加载一个 <code>Dataset</code> 对象：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token comment"># 创建自定义数据集</span>
custom_dataset <span class="token operator">=</span> MyCustomDataset<span class="token punctuation">(</span>data<span class="token punctuation">,</span>csv_file<span class="token punctuation">,</span> root_dir<span class="token punctuation">)</span>

<span class="token comment"># 使用 DataLoader 加载数据</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>custom_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 迭代 DataLoader 并打印数据和标签</span>
<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-集成的数据集" tabindex="-1"><a class="header-anchor" href="#_2-集成的数据集"><span>2. 集成的数据集</span></a></h3><blockquote><p><code>torchvision.datasets</code></p></blockquote><p><code>torchvision</code> 是 PyTorch 中的一个视觉处理库，提供了数据集、图片变换等一系列图像处理操作。</p><p><code>torchvision.datasets</code> 中包含了一些常用的数据集，例如 <code>MNIST、CIFAR10、CIFAR100、ImageNet</code> 等。这些数据集通常是用于测试和比较不同模型的性能，同时也可以用于训练模型。</p><p>使用 <code>torchvision.datasets</code> 可以方便地下载、加载和预处理这些常用数据集。常用函数和参数如下：</p><ul><li><p><code>torchvision.datasets.MNIST</code></p><p>：加载 MNIST 数据集。</p><ul><li><code>root</code>：MNIST 数据集的根目录。</li><li><code>train</code>：选择是否加载训练集，默认为 True。</li><li><code>transform</code>：数据预处理函数，例如缩放、剪裁、随机切割等。</li><li><code>target_transform</code>：标签数据的预处理函数。</li><li><code>download</code>：是否下载 MNIST 数据集。如果数据已经下载过，则会直接读取本地数据。</li></ul></li><li><p><code>torchvision.datasets.CIFAR10</code>：加载 CIFAR10 数据集，参数和 MNIST 数据集类似。</p></li></ul><p>例如，以下代码给出了如何使用 <code>torchvision.datasets</code> 加载 MNIST 数据集并进行数据预处理：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>datasets <span class="token keyword">as</span> datasets 
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token comment"># 数据预处理，将输入数据放缩到 (0, 1) 范围内 </span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>    
    <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>     
     transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># 下载和加载数据集 </span>
train_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">&#39;data/MNIST&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span> test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span><span class="token string">&#39;data/MNIST&#39;</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先定义了一个数据预处理函数 <code>transform</code>，将输入数据放缩到 (0, 1) 范围内。然后，我们使用 <code>datasets.MNIST</code> 来下载和加载 MNIST 数据集，其中传入了一些参数，例如 <code>root</code> 表示数据存放的根目录，<code>train</code> 表示加载训练数据集，<code>transform</code> 是数据预处理函数。</p><p>最后，我们得到了 <code>train_data</code> 和 <code>test_data</code> 两个数据集对象，分别用于模型的训练和测试。</p><h2 id="二、优化算法" tabindex="-1"><a class="header-anchor" href="#二、优化算法"><span>二、优化算法</span></a></h2><h3 id="_1-训练优化中的问题" tabindex="-1"><a class="header-anchor" href="#_1-训练优化中的问题"><span>1. 训练优化中的问题</span></a></h3><p><strong>局部最小值</strong></p><p>深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数<em>局部</em>最优，而不是<em>全局</em>最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。</p><figure><img src="/dcblog/assets/local_minimum-BYhjLrU-.svg" alt="../_images/output_optimization-intro_70d214_51_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_51_0.svg</figcaption></figure><p><strong>鞍点</strong></p><p>除了局部最小值之外，鞍点是梯度消失的另一个原因。<em>鞍点</em>（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数f(x)=x^3。它的一阶和二阶导数在x=0时消失。这时优化可能会停止，尽管它不是最小值。</p><figure><img src="/dcblog/assets/saddle-vClhm6ni.svg" alt="../_images/output_optimization-intro_70d214_66_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_66_0.svg</figcaption></figure><p><strong>梯度消失</strong></p><p>假设我们想最小化函数$f(x)=tanh⁡(x)$，然后我们恰好从$x=4$开始。正如我们所看到的那样，f的梯度接近零。更具体地说，$f′(x)=1−tanh^2⁡(x)$，因此是$f′(4)=0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。</p><figure><img src="/dcblog/assets/gradient-tiKjipcT.svg" alt="../_images/output_optimization-intro_70d214_96_0.svg" tabindex="0" loading="lazy"><figcaption>../_images/output_optimization-intro_70d214_96_0.svg</figcaption></figure><h3 id="_2-优化算法" tabindex="-1"><a class="header-anchor" href="#_2-优化算法"><span>2. 优化算法</span></a></h3><p><strong>随机梯度下降</strong></p><blockquote><p><code>torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code></p></blockquote><p><code>torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)</code> 是 PyTorch 中的一个优化器类，实现了随机梯度下降（SGD）算法。该算法是常用的优化算法之一，通过计算当前梯度和历史梯度的加权平均值来更新模型参数。</p><ul><li><code>params</code>：待优化的参数。</li><li><code>lr</code>：学习率，控制每次更新的步长大小。</li><li><code>momentum</code>：动量（Momentum）因子，加速优化过程，防止参数在更新方向改变时来回摆动。默认值为 0。</li><li><code>dampening</code>：动量的抑制因子，范围为 [0, 1]。默认值为 0。</li><li><code>weight_decay</code>：权重衰减因子，用于控制模型复杂度。默认为 0，表示不使用权重衰减。</li><li><code>nesterov</code>：是否使用 Nesterov 加速梯度，即使用当前位置的动量项进行梯度估计，可以提高优化效果。默认值为 False。</li></ul><p>例如，以下代码给出了如何使用 <code>torch.optim.SGD</code> 将模型的权重向量 w 进行随机梯度下降更新：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim

model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> labels <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们首先定义了一个模型 <code>MyModel</code>，然后使用 <code>model.parameters()</code> 获取模型的参数，传入 <code>optim.SGD()</code> 函数中创建一个 <code>SGD</code> 优化器对象 <code>optimizer</code>。在模型的训练过程中，我们使用 <code>optimizer.zero_grad()</code> 来清空历史梯度，然后进行前向传播和反向传播，最后通过 <code>optimizer.step()</code> 来更新模型的参数。</p><h2 id="三、网络层结构" tabindex="-1"><a class="header-anchor" href="#三、网络层结构"><span>三、网络层结构</span></a></h2><h3 id="_1-展平层" tabindex="-1"><a class="header-anchor" href="#_1-展平层"><span>1. 展平层</span></a></h3><p><code>nn.Flatten()</code> 是 PyTorch 中的一个层（layer），用于将输入张量展平成一个向量，例如将一个大小为 <code>(n, c, h, w)</code> 的四维张量展平为一个大小为 <code>(n, c*h*w)</code> 的二维张量。</p><p>这个操作在神经网络中经常用到，通常出现在卷积和池化层后以便将其输出进入全连接层。具体来说，当卷积和池化层的输出为四维张量，而全连接层的输入为二维张量时，这时就需要使用 <code>nn.Flatten()</code> 来将张量展平为一个向量。</p><p>例如，以下代码演示了如何使用 <code>nn.Flatten()</code>：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span> 
<span class="token punctuation">)</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的代码中，我们定义了一个简单的 CNN 模型，其中包含了两个卷积层、两个 ReLU 激活层和两个 MaxPool 层。这些层的输出都是四维张量，因此我们需要在最后一个 MaxPool2d 层之后使用 <code>nn.Flatten()</code> 层将输出张量展平成二维张量，然后才能进入全连接层进行分类。</p><h3 id="_2-卷积层" tabindex="-1"><a class="header-anchor" href="#_2-卷积层"><span>2. 卷积层</span></a></h3><p><code>nn.Conv2d</code>是PyTorch中用于定义二维卷积层的函数。它是基于torch.nn.Module构造出的类，可以直接使用PyTorch提供的API进行调用，其主要参数包括：输入通道数，输出通道数，卷积核大小，步长、填充等。当定义了卷积层后，需要调用forward函数计算输出值。</p><p>例如，下面是一个使用nn.Conv2d定义卷积层的简单示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token comment"># 定义卷积层</span>
conv_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 假设输入一个大小为 [batch_size, 3, 28, 28] 的张量 x</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>

<span class="token comment"># 计算卷积层输出</span>
output <span class="token operator">=</span> conv_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token comment"># 输出结果的大小: [batch_size, 64, 28, 28]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这个示例中，定义了一个输入通道数为3，输出通道数为64，卷积核大小为3x3，步长、填充均为1的卷积层，并将其应用到一个大小为[16, 3, 28, 28]的张量上，然后输出的结果大小是[16, 64, 28, 28]，其中16是batch_size的大小。</p><h3 id="_3-池化层" tabindex="-1"><a class="header-anchor" href="#_3-池化层"><span>3. 池化层</span></a></h3><blockquote><p><code>nn.MaxPool2d</code></p></blockquote><p><code>nn.MaxPool2d</code>是PyTorch神经网络库中提供的最大池化层。它可以在输入上执行2D最大池化操作，将输入划分为固定的池化区域，并在每个池化区域内找到最大值。具体而言，MaxPool2d的作用是将输入张量在每个池化窗口内取最大值，并将所有结果组合成一个张量，作为输出。</p><p>该函数有以下参数：</p><ul><li><code>kernel_size</code>: 池化窗口的大小（宽度，高度）。</li><li><code>stride</code>： 池化窗口的移动步长。</li><li><code>padding</code>：对高度和宽度的零填充数量</li><li><code>dilation</code>: 控制滤波器元素之间距离的参数。</li><li><code>return_indices</code>：返回由最大值组成的张量的位置的指标，而不是最大值本身。</li><li><code>ceil_mode</code>：是否输出图像大小向上取整</li></ul><p>下面是一个示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># Define a 2D max pooling layer</span>
maxpool_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># Apply max pooling to an input tensor</span>
input_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output_tensor <span class="token operator">=</span> maxpool_layer<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Input shape: </span><span class="token interpolation"><span class="token punctuation">{</span>input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Output shape: </span><span class="token interpolation"><span class="token punctuation">{</span>output_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码定义了一个kernel大小为2 * 2，步幅为2的2D最大池化层，并将该层应用于一个张量。输入张量的形状为（1，3，4，4），其中1表示批次大小，3表示通道数，4和4表示输入图像的宽度和高度。在这种情况下，MaxPool2d的输出张量形状为（1，3，2，2），因为窗口中的4个池化区域中的每个都有一个输出值。在这个示例中，不使用其他参数，因此默认参数被用到。</p><hr><blockquote><p><code>nn.AdaptiveAvgPool2d</code></p></blockquote><p><code>nn.AdaptiveAvgPool2d</code>是PyTorch神经网络库中提供的自适应平均池化层。它对输入进行二维平均池化，使输出具有特定形状。它可以自适应地考虑更大或更小的输入张量，并根据指定的输出大小调整窗口大小和步幅，以使输出具有期望的大小。</p><p>这个函数有一个必须的参数输出大小（output_size），我们希望输出的张量大小为（batch_size，channel，output_size，output_size）</p><p><code>nn.AdaptiveAvgPool2d</code>的输出张量形状为指定大小的输出，无论输入张量的尺寸如何，它会自动调整窗口大小和步幅。</p><p>下面是一个示例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># Define a 2D adaptive average pooling layer</span>
adaptive_avg_pool_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># Apply adaptive average pooling to an input tensor</span>
input_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output_tensor <span class="token operator">=</span> adaptive_avg_pool_layer<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Input shape: </span><span class="token interpolation"><span class="token punctuation">{</span>input_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&quot;Output shape: </span><span class="token interpolation"><span class="token punctuation">{</span>output_tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">&quot;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码定义了一个2D自适应平均池化层，并将其应用于一个输入张量。输入张量的形状为（1，3，4，4），其中1表示批次大小，3表示通道数，4和4表示输入图像的宽度和高度。 在这个示例中，输出大小为（2,2），因此输出张量的形状为（1，3，2，2）。</p><h3 id="_4-dropout" tabindex="-1"><a class="header-anchor" href="#_4-dropout"><span>4. Dropout</span></a></h3><p><code>dropout</code>是一种常用的正则化方法用于减少神经网络的过拟合风险。 在运行时期间，<code>dropout</code>以一定的概率丢弃（设置于0）输入单元或输出单元的权重，然后从训练数据集的小批量数据中随机选择新的输入单元或输出单元的权重。 过拟合风险较大的神经元将会有较大的概率被丢弃，保留较小的概率，以减轻神经元之间的复杂协同关系。</p><p>在深度学习网络训练过程中，<code>dropout</code>层一般加在全连接层之后，可以通过<code>nn.Dropout(p = probability)</code>直接实现。p为丢弃的概率，通常取0.2~0.5之间的值。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_5-batchnorm" tabindex="-1"><a class="header-anchor" href="#_5-batchnorm"><span>5. BatchNorm</span></a></h3><p><code>nn.BatchNorm2d</code>是PyTorch中的一个批量归一化（Batch Normalization）操作类。它会对输入进行标准化处理，让前一层的输出其均值为0，方差为1，并通过可学习的缩放和偏移参数，使模型能够恢复任意的变换。</p><p>该类的主要参数包括：</p><ul><li><code>num_features</code>：输入特征的通道数。</li><li><code>eps</code>：为了避免分母为零的情况，在分母上加上一个很小的数 eps。</li><li><code>momentum</code>：用于计算移动平均值的动量系数，用于更新求得的均值和方差。</li><li><code>affine</code>: bool类型，默认值为True，表示是否启用可学习的缩放和平移参数。</li><li><code>track_running_stats</code>: bool类型，默认值为True，表示是否使用运行统计信息（如平均值和标准差）来标准化。</li></ul><p>下面是一个简单的使用实例：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 初始化一个 BatchNorm2d</span>
bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>num_features<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>

<span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> bn<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>该例子中，<code>num_features</code>被设置为3，表示输入特征的通道数为3。输入的张量<code>input</code>的形状为(2, 3, 4, 4)，表示batch_size为2，通道数为3，每个特征图的大小为(4, 4)。执行<code>bn(input)</code>后，返回的输出张量<code>output</code>具有相同的形状。</p><p>在训练过程中，<code>BatchNorm2d</code> 对接收到的每个 <code>feature map</code> 分别执行以下操作：</p><ol><li>计算每个 feature map 的均值和方差，并通过移动平均的方式整合到全局均值和方差中。</li><li>根据上述的均值和方差，对 feature map 进行标准化处理。</li><li>通过可学习的缩放和偏移参数进行线性变换。</li><li>返回标准化后的 feature map。</li></ol><p>需要注意的是，在测试模式下，BatchNorm2d 的运作方式与训练模式不同，其前向传播不会使用 mini-batch 的均值和方差，而是使用全局的均值和方差（在训练过程中通过动量方式累积得到的均值和方差）来进行标准化处理。</p><h3 id="_6-relu" tabindex="-1"><a class="header-anchor" href="#_6-relu"><span>6. Relu</span></a></h3><p><code>nn.ReLU</code>是对输入的数据进行修正线性单元(Rectified Linear Units, ReLU)操作的类。ReLU是深度学习中常用的激活函数，它可以通过将负值变为0来增强线性神经网络的非线性性。其代数式为$f(x) = max(0,x)$，即当输入x大于0时$f(x)$等于$x$，否则$f(x)$等于0。</p><p><code>nn.ReLU</code>的功能很简单，它接收输入张量并将其作用于ReLU激活函数，输出ReLU激活后的张量。在pytorch中使用<code>nn.ReLU</code>的代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> relu<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中<code>input</code>是输入张量，<code>output</code>是经过ReLU激活后的输出张量。</p><blockquote><p><code>nn.ReLU(inplace=True)</code></p></blockquote><p><code>nn.ReLU(inplace=True)</code>与<code>nn.ReLU()</code>的区别在于，它将原地修改输入张量而不是创建一个新的张量作为输出，并返回已修改的输入张量。</p><p>这意味着，使用<code>nn.ReLU(inplace=True)</code>会省略内存分配和释放，因为不会产生新的内存分配和释放操作。这在处理大量数据时，可以有效地减少内存开销，因为每次内存分配和释放都需要消耗大量的时间和资源。同时，使用inplace的操作还可以提高计算性能，因为不需要将数据从一个位置复制到另一个位置。</p><p>需要注意的是，使用<code>inplace=True</code>有时也可能会导致不可预测的结果或者梯度计算的错误。因此，在使用时需要谨慎，并根据具体情况判断是否需要开启inplace模式。</p><p><code>nn.ReLU</code>可能存在的问题有两个：</p><ol><li>梯度消失或爆炸：当输入很大或很小的时候，ReLU的导数会变成0或无穷大，从而导致梯度消失或爆炸的问题。</li><li>神经元死亡：如果训练期间一个ReLU神经元被更新成一直输出0的状态，那么在以后的训练过程中该神经元的参数将一直保持不变，也就是被&quot;死亡&quot;了。这种情况的发生通常与学习率过高有关，可以通过减小学习率或使用其他激活函数来缓解。</li></ol><h3 id="_7-反卷积层" tabindex="-1"><a class="header-anchor" href="#_7-反卷积层"><span>7. 反卷积层</span></a></h3><p><code>nn.ConvTranspose2d</code> 是 PyTorch 中的一个卷积转置层，用于进行反卷积（Deconvolution）操作。它的作用是将一个低维的特征图映射到一个高维空间中，可以用于图像分割、语义分割、超分辨率等任务。</p><p>在具体实现中，<code>nn.ConvTranspose2d</code> 与 <code>nn.Conv2d</code> 的实现方式十分相似，都是通过卷积核与输入的特征图进行卷积操作，这里不再赘述。主要的区别在于反卷积操作是将卷积核中的值进行上采样之后进行卷积，从而实现特征图的上采样，对应于卷积操作中的下采样。</p><p>具体来说， <code>nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)</code> 的参数含义介绍如下：</p><ul><li><code>in_channels</code> : 输入特征图的通道数。</li><li><code>out_channels</code> : 输出特征图的通道数。</li><li><code>kernel_size</code> : 卷积核的大小。</li><li><code>stride</code> : 步长，默认值为 1。</li><li><code>padding</code> : 填充大小，默认值为 0。</li><li><code>output_padding</code> : 输出特征图填充大小，默认值为 0。</li><li><code>groups</code> : 组数，默认值为 1。一个 ConvTranspose2d 连接了多个输入通道组和多个输出通道组。</li><li><code>bias</code> : 是否使用偏置，默认为 <code>True</code>。</li><li><code>dilation</code> : 卷积核的膨胀率，默认为 1。</li></ul><p>使用方法如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

conv_trans <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token comment"># 在输入特征图上进行反卷积</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> conv_trans<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输出形状为 [1, 64, 25, 25]</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这里，我们首先创建了一个反卷积层 <code>conv_trans</code>，将输入通道数设为 32，输出通道数设为 64，卷积核大小为 (3, 3)。接着，我们使用 PyTorch 中的 <code>torch.randn()</code> 函数创建了一个大小为 (1, 32, 12, 12) 的输入特征图，其中 1 表示 batch_size，32 表示输入通道数，12 表示特征图高和宽。最后，我们对输入图像做了反卷积操作，输出的特征图大小为 (1, 64, 25, 25)，其中特征图高和宽分别为 25，可以根据卷积转置的计算公式推导来进行理解。</p><h2 id="四、参数访问" tabindex="-1"><a class="header-anchor" href="#四、参数访问"><span>四、参数访问</span></a></h2><h3 id="_1-named-parameters-或parameters" tabindex="-1"><a class="header-anchor" href="#_1-named-parameters-或parameters"><span>1. <code>named_parameters()</code>或<code>parameters()</code></span></a></h3><p>使用model.named_parameters()或model.parameters()方法，返回一个生成器，遍历每个参数并返回对应的参数张量及名称，代码示例如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token operator">*</span><span class="token number">8</span><span class="token operator">*</span><span class="token number">8</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># named_parameters()方法获取网络层参数及名称</span>
<span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
     <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><code>model.named_parameters</code>访问的是所有层的参数，要想访问具体某一层的参数，只需要<code>model.(层name).named_parameters</code>.</p></blockquote><h3 id="_2-model-layer-weight" tabindex="-1"><a class="header-anchor" href="#_2-model-layer-weight"><span>2. <code>model.layer.weight</code></span></a></h3><p>使用<code>model.layer.weight</code>和<code>model.layer.bias</code>直接获取某一层的权重和偏置，示例代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 直接访问module的成员变量获取网络层参数</span>
weight <span class="token operator">=</span> model<span class="token punctuation">.</span>layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data
bias <span class="token operator">=</span> model<span class="token punctuation">.</span>layer<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data
<span class="token keyword">print</span><span class="token punctuation">(</span>weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>直接使用<code>model.layer.weight</code>，返回值除了值外，还包括梯度等额外信息。</li></ul><h3 id="_3-state-dict" tabindex="-1"><a class="header-anchor" href="#_3-state-dict"><span>3. <code>state_dict</code></span></a></h3><p>使用model.state_dict()方法获取网络层的参数字典，该字典包含每个层的名称和参数，例如<code>model.state_dict()[&#39;layer.weight&#39;]</code>，示例代码如下：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch

<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

model <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># state_dict方法获取网络层参数</span>
params_dict <span class="token operator">=</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>
weight <span class="token operator">=</span> params_dict<span class="token punctuation">[</span><span class="token string">&#39;layer.weight&#39;</span><span class="token punctuation">]</span>
bias <span class="token operator">=</span> params_dict<span class="token punctuation">[</span><span class="token string">&#39;layer.bias&#39;</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>下面是<code>state_dict()</code>返回的结果样例</p></blockquote><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">&#39;weight&#39;</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3016</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1901</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1991</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1220</span><span class="token punctuation">,</span>  <span class="token number">0.1121</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1424</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3060</span><span class="token punctuation">,</span>  <span class="token number">0.3400</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&#39;bias&#39;</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0291</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="五、保存和读取模型参数" tabindex="-1"><a class="header-anchor" href="#五、保存和读取模型参数"><span>五、保存和读取模型参数</span></a></h2><h3 id="_1-保存和读取" tabindex="-1"><a class="header-anchor" href="#_1-保存和读取"><span>1. 保存和读取</span></a></h3><p>PyTorch 中，我们可以使用 <code>state_dict</code> 来保存和读取模型的参数。<code>state_dict</code> 是一个 Python 字典对象，它将每个层映射到其参数张量。可以将 <code>state_dict</code> 存储为文件以备以后使用，也可以将其作为 Python 对象传递给其他 PyTorch 模型。</p><p>下面是一个简单的示例，说明如何保存和读取模型参数：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

<span class="token comment"># 定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">MyModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>MyModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 保存模型参数</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&#39;model_params.pth&#39;</span><span class="token punctuation">)</span>

<span class="token comment"># 加载模型参数</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;model_params.pth&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在这个示例中，我们首先定义了一个简单的模型 <code>MyModel</code>，它包含一个线性层。接下来，我们通过调用 <code>torch.save</code> 函数，将模型的 <code>state_dict</code> 保存到名为 <code>&#39;model_params.pth&#39;</code> 的文件中。</p><p>最后，我们使用 <code>torch.load</code> 函数加载保存的 <code>state_dict</code>。然后调用 <code>model.load_state_dict</code> 将加载的 <code>state_dict</code> 传递给模型。这将更新模型的参数。</p><p>PyTorch 也支持 <code>save</code> 和 <code>load</code> 函数，这些函数将整个模型保存到文件中，并支持在 Python 和 C++ 中加载模型。这些函数在处理多个对象时更具有灵活性，但 <code>state_dict</code> 更适用于保存和加载模型参数。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>mydict <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&#39;x&#39;</span><span class="token punctuation">:</span> x<span class="token punctuation">,</span> <span class="token string">&#39;y&#39;</span><span class="token punctuation">:</span> y<span class="token punctuation">}</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>mydict<span class="token punctuation">,</span> <span class="token string">&#39;mydict&#39;</span><span class="token punctuation">)</span>
mydict2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;mydict&#39;</span><span class="token punctuation">)</span>
mydict2
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token punctuation">{</span><span class="token string">&#39;x&#39;</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">&#39;y&#39;</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h3 id="_2-查看数据结构" tabindex="-1"><a class="header-anchor" href="#_2-查看数据结构"><span>2. 查看数据结构</span></a></h3><p>使用torch.save()函数保存的模型权重参数文件默认是以Python的pickle格式进行序列化的，包括datasets中数据集默认也是以这种方式进行的存储，这种文件直接使用文本打开会显示乱码，我们可以利用pickle对文件进行反序列化，以查看其数据结构：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> pickle
filename <span class="token operator">=</span> <span class="token string">&#39;../cifar10/cifar-10-batches-py/test_batch&#39;</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>filename<span class="token punctuation">,</span><span class="token string">&#39;rb&#39;</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    dataset <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">&#39;bytes&#39;</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>dataset<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
<span class="token comment"># out: &lt;class &#39;dict&#39;&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="六、gpu相关" tabindex="-1"><a class="header-anchor" href="#六、gpu相关"><span>六、GPU相关</span></a></h2><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>nvidia-smi
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ul><li>使用<code>nvidia-smi</code>命令来查看显卡信息</li></ul><figure><img src="/dcblog/assets/image-20230329155746146-B91yW8yb.png" alt="image-20230329155746146" tabindex="0" loading="lazy"><figcaption>image-20230329155746146</figcaption></figure><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cpu&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment"># cuda和cuda:0等价</span>
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda:1&#39;</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;cuda:</span><span class="token interpolation"><span class="token punctuation">{</span>i<span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span> 

<span class="token comment"># 常用</span>
device <span class="token operator">=</span> <span class="token string">&#39;cuda&#39;</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">&#39;cpu&#39;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-将模型参数放在gpu" tabindex="-1"><a class="header-anchor" href="#_1-将模型参数放在gpu"><span>1. 将模型参数放在gpu</span></a></h3><ol><li><p>在创建的时候指定device</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用cuda()复制</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>Z <span class="token operator">=</span> X<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 参数为cuda的编号</span>
Z
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div></li><li><p>使用to()指定</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>net <span class="token operator">=</span> net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token operator">=</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&#39;cuda&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div></li></ol><h3 id="_2-切换模型模式" tabindex="-1"><a class="header-anchor" href="#_2-切换模型模式"><span>2. 切换模型模式</span></a></h3><ul><li>切换：<code>model.train()和model.eval()</code></li><li>判断：<code>torch.is_grad_enabled()</code><ul><li>检查当前PyTorch计算图是否启用梯度计算。它返回一个布尔值，如果梯度计算启用，则返回True，否则返回False。</li></ul></li></ul><h2 id="七、注意力机制" tabindex="-1"><a class="header-anchor" href="#七、注意力机制"><span>七、注意力机制</span></a></h2><p>灵长类动物的视觉系统接受了大量的感官输入， 这些感官输入远远超过了大脑能够完全处理的程度。 然而，并非所有刺激的影响都是相等的。 意识的聚集和专注使灵长类动物能够在复杂的视觉环境中将注意力引向感兴趣的物体，例如猎物和天敌。 只关注一小部分信息的能力对进化更加有意义，使人类得以生存和成功。</p><p>在注意力机制中，会使用到三个重要的概念：查询(Query)、键(Keys)和值(Value)。</p><ul><li><p>查询(Query)：代表我们要对输入中的哪一部分进行关注和处理，通常是在计算注意力权重时所依据的向量。它是通过上一步的输出来计算的，具体的计算方式取决于注意力机制的不同类型。</p></li><li><p>键(Keys)：它是用来表示输入序列中各个位置的特征的向量，一般是通过对输入进行变换（如全连接层或卷积操作）获得。</p></li><li><p>值(Value)：它是输入序列中的各个位置所对应的实际数值（训练时的输出y），是未经过变换的原始输入。</p></li></ul><p>在注意力机制中，我们做的实际上是以下3件事：</p><ul><li>首先将查询向量与键向量计算余弦相似度或欧式距离（评分函数） <ul><li>越小代表键靠近自己想查询的值，随之计算出的权重越大</li></ul></li><li>进行 softmax 激活后得到注意力权重</li><li>最后将注意力权重与值向量进行加权求和</li></ul><p>这个操作的目的是对不同的输入位置赋予不同的权重，使得在对序列进行处理时，对最相关和最重要的部分给予更大的关注和权重，从而提高序列的表示能力。</p><figure><img src="/dcblog/assets/attention-output-CwOucdKr.svg" alt="../_images/attention-output.svg" tabindex="0" loading="lazy"><figcaption>../_images/attention-output.svg</figcaption></figure><h2 id="八、一些通用函数" tabindex="-1"><a class="header-anchor" href="#八、一些通用函数"><span>八、一些通用函数</span></a></h2><h3 id="_1-训练函数" tabindex="-1"><a class="header-anchor" href="#_1-训练函数"><span>1. 训练函数</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">def</span> <span class="token function">train_ch6</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 初始化权重</span>
    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear <span class="token keyword">or</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;training on:&#39;</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token comment"># 优化器</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    <span class="token comment"># 损失函数</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">&#39;epoch&#39;</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>
                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&#39;train loss&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;train acc&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;test acc&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    timer<span class="token punctuation">,</span> num_batches <span class="token operator">=</span> Timer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span> <span class="token comment">#迭代次数</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 训练损失之和，训练准确率之和，样本数</span>
        metric <span class="token operator">=</span> Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
            timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_l <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            train_acc <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_batches <span class="token operator">//</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> i <span class="token operator">==</span> num_batches <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_batches<span class="token punctuation">,</span>
                             <span class="token punctuation">(</span>train_l<span class="token punctuation">,</span> train_acc<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> evaluate_accuracy_gpu<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>
        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;loss: </span><span class="token interpolation"><span class="token punctuation">{</span>train_l<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, train acc: </span><span class="token interpolation"><span class="token punctuation">{</span>train_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, &#39;</span></span>
          <span class="token string-interpolation"><span class="token string">f&#39;test acc: </span><span class="token interpolation"><span class="token punctuation">{</span>test_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f&#39;</span><span class="token interpolation"><span class="token punctuation">{</span>metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> num_epochs <span class="token operator">/</span> timer<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string"> examples/sec &#39;</span></span>
          <span class="token string-interpolation"><span class="token string">f&#39;on </span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">&#39;</span></span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div><!--[--><!----><!--]--><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/dcbestwords/dcBlog/edit/main/src/paper/code/torch_api.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><div class="update-time"><span class="label">上次编辑于: </span><!----></div><div class="contributors"><span class="label">贡献者: </span><!--[--><!--[--><span class="contributor" title="email: 1114686398@qq.com">dachao</span><!--]--><!--]--></div></div></footer><!----><!----><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">默认页脚</div><div class="vp-copyright">Copyright © 2024 Dachao </div></footer></div><!--]--><!--[--><!----><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/dcblog/assets/app-BPd7oWPf.js" defer></script>
  </body>
</html>
